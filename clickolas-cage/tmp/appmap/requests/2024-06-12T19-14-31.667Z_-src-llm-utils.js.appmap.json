{
  "events": [
    {
      "event": "call",
      "http_server_request": {
        "path_info": "/src/llm-utils.js",
        "request_method": "GET",
        "headers": {
          "Host": "localhost:5173",
          "Connection": "keep-alive",
          "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
          "Accept": "*/*",
          "Sec-Fetch-Site": "none",
          "Sec-Fetch-Mode": "cors",
          "Sec-Fetch-Dest": "empty",
          "Accept-Encoding": "gzip, deflate, br, zstd",
          "Accept-Language": "en-US,en;q=0.9",
          "Cookie": "appland_session=bFU5p0cvqrcXlz2oQfgEZG3%2BQjfds2AwmlOA%2Fx5fhQLjPvVHIKrHyvnbeq5nw8c5sME56znPsnpSSzB3nr%2BHfdk9sEC9MgOgNKjgjD8qlzGqDVuwRPr30ch96LWGIDwuITrAUwQGqlUw7%2Fdsq0tjrfHFPTlemq71kQtCAIi9JTRWo5tLPJlG%2F9JtfTuiWIfWg8X67ZkAOuaapIHdaA5w5QtwMr7h34LfseImCERQpzoz9xTqJYqSi2SJ9RE1eVw2jyYFG9IGPA4d1USUIXvAmeaMOQ9Z%2BDmKhiIRXve6%2BzO47%2F2RW1kaH2PND2r0EKz%2BaWMbrXswEaj2kjwx09pTJANR5arO0I9c2RelyrURJyvI%2FZ8OwnG99VyxbE2scovqFvxwhmhXr1KDOxpRle3VOJSpxXtFaDjv2TSSbtqGkIj4T51u58xasM4KbZUpapu5OQ5ENf%2FSMADW4DHmCrSQJ14SgVbNtrazlgwdph5p0AKax1l0Bw7j0jz2SGT9C5HMnzVyZdV59G6paFrgLqvtGuXdWr4VeJt2tPVftjki2S0LpmpTAacYMTeiz0JRvAUI2MTy5coFmozGwP2tYolztMsGb%2Bss8Pp0ZRYy8EsEs78L1J1nDMR5VCJ5yVhSq8JXZA7Misee05vblyt4OMHf9CA4AMzWpe2NbExRxiHuW%2Bkx%2F83vj%2Btrqn1QRG33cQUOn3jl8nfSuSLj5p0K5PYVYnXJu8E7RhAyqpC6J23a%2F8B4qsDAOrIrUYPQAWaYOsjd0i3Q5y49d77RlVc7zyyLJxFNVWLlYCD5BrfOMpyF2f9InowPEQei23xjT4UE0W3dXZP1xhk5wDvQ--zbb2b%2Flld908TQxF--8LcY2kn8%2BBw8bSaDiyP6LQ%3D%3D"
        },
        "protocol": "HTTP/1.1"
      },
      "id": 1,
      "thread_id": 0,
      "message": [
        {
          "name": "t",
          "value": "'1718219671644'",
          "class": "String"
        }
      ]
    },
    {
      "event": "return",
      "http_server_response": {
        "status_code": 200,
        "headers": {
          "Access-Control-Allow-Origin": "*",
          "Content-Type": "application/javascript",
          "Cache-Control": "no-cache",
          "Etag": "W/\"2d9a-DuofQl05l5kJrRQucW/ztAPp+9I\""
        },
        "return_value": {
          "class": "[ResponseBody]",
          "value": "import.meta.env = {\"VITE_OPENAI_API_KEY\":\"sk-yourkey\",\"VITE_GEMINI_API_KEY\":\"AIzaSyCc9dZOx9-47p_MSktvD9tu1jbxGJ5_rMI\",\"BASE_URL\":\"/\",\"MODE\":\"development\",\"DEV\":true,\"PROD\":false,\"SSR\":false};import OpenAI from \"/node_modules/.vite/deps/openai.js?v=85b4035a\"\nimport __vite__cjsImport1_portkeyAi from \"/node_modules/.vite/deps/portkey-ai.js?v=85b4035a\"; const PORTKEY_GATEWAY_URL = __vite__cjsImport1_portkeyAi[\"PORTKEY_GATEWAY_URL\"]; const createHeaders = __vite__cjsImport1_portkeyAi[\"createHeaders\"]\nconst model = 'gemini-1.5-flash-latest'\n\nconst openai = new OpenAI({\n  // apiKey: 'not-needed', // defaults to process.env[\"\"]\n  apiKey: import.meta.env.VITE_GEMINI_API_KEY, // defaults to process.env[\"\"]\n  // apiKey: \"\",\n  baseURL: 'http://localhost:8787/v1',\n  dangerouslyAllowBrowser: true,\n  defaultHeaders: createHeaders({\n    provider: 'google',\n  }),\n})\n\n/**\n * Wrapper function for OpenAI chat completion calls with logging.\n * @param {Object} messages - The messages payload to send to the OpenAI API.\n * @returns {Promise<Object>} - The response from the OpenAI API.\n */\nconst openAiChatCompletionWithLogging = async (messages) => {\n  chrome.storage.local.get({ logs: [] }, (result) => {\n    const logs = result.logs\n    logs.push({ messages })\n    chrome.storage.local.set({ logs })\n  })\n  const response = await openAiCallWithRetry(() =>\n    openai.chat.completions.create({\n      model: model,\n      seed: 1,\n      response_format: { type: 'json_object' },\n      messages: messages,\n    }),\n  )\n  chrome.storage.local.get({ logs: [] }, (result) => {\n    const logs = result.logs\n    logs.push({ response })\n    chrome.storage.local.set({ logs })\n  })\n  return response\n}\n\n/**\n * Extracts the first JSON object from a given string.\n * @param {string} str - The string to search for a JSON object.\n * @returns {Object|null} The parsed JSON object, or null if no valid JSON object is found.\n */\nconst extractJsonObject = (str) => {\n  // Regular expression to match JSON objects\n  const jsonRegex = /{(?:[^{}]|{(?:[^{}]|{[^{}]*})*})*}/g\n\n  // Find the first match in the string\n  const match = str.match(jsonRegex)\n\n  if (match) {\n    try {\n      // Parse the matched string into a JSON object\n      return JSON.parse(match[0])\n    } catch (e) {\n      // Handle parsing error\n      console.error('Error parsing JSON: ', e)\n      return null\n    }\n  } else {\n    // No JSON found in the string\n    return null\n  }\n}\n\n/**\n * Makes an OpenAI API call with retry logic.\n * @param {Function} call - The OpenAI API call function to execute.\n * @param {number} [retryCount=3] - The number of times to retry the API call if it fails.\n * @returns {Promise} A promise that resolves with the response from the successful API call.\n */\nasync function openAiCallWithRetry(call, retryCount = 3) {\n  for (let i = 0; i < retryCount; i++) {\n    try {\n      const response = await call()\n      return response\n    } catch (error) {\n      console.error(`Attempt ${i + 1} failed with error: ${error}`)\n      if (i === retryCount - 1) throw error\n    }\n  }\n}\n\n/**\n * Sends a prompt to the OpenAI API with user feedback and returns a revised plan.\n * @param {string} originalPrompt - The original prompt given to the AI.\n * @param {string} originalPlan - The original plan generated by the AI.\n * @param {string} currentStep - The current step of the plan being executed.\n * @param {string} feedback - User feedback on the current step.\n * @returns {Promise<Object>} A revised plan based on the user feedback.\n */\nexport const sendPromptWithFeedback = async (\n  originalPrompt,\n  originalPlan,\n  currentStep,\n  feedback,\n) => {\n  const chatCompletion = await openAiChatCompletionWithLogging([\n    {\n      role: 'system',\n      content: `you are an expert web browsing AI. you were given the original prompt:\"${originalPrompt}\"\nyou originally came up with the plan:\n${originalPlan}\nWe currently just tried to execute step of the plan :\n${currentStep}\n  given user feedback, come up with a revised plan from the current step.\n  Provide a response with this JSON schema:\n{\n  plan: [ {\n    thought: \"one sentence rationale\",\n    action: \"NAVURL\" | \"CLICKBTN\" | \"INPUT\" | \"SELECT\" | \"WAITLOAD\" ,\n    ariaLabel: \"labelName\",\n    param?: \"url\" | \"inputOption\" | \"inputText\"\n  },...]\n} `,\n    },\n    {\n      role: 'user',\n      content: `user has answered the question with ${feedback}`,\n    },\n  ])\n  console.log(chatCompletion.choices[0].message.content)\n  return extractJsonObject(chatCompletion.choices[0].message.content)\n}\n\n/**\n * Sends a prompt to the Plan Reviser to get a revised plan based on the current step and available text options.\n * @param {string} originalPrompt - The original prompt given to the AI.\n * @param {string} originalPlan - The original plan created by the AI.\n * @param {string} currentStep - The current step being executed.\n * @param {any[]} textOptions - An array of user-provided node aria-labels.\n * @returns {Promise<Object>} - A promise that resolves to the revised plan in JSON format.\n */\nexport const getNextStepFromLLM = async (\n  originalPrompt,\n  currentURL,\n  originalPlan,\n  currentStep,\n  textOptions,\n) => {\n  const chatCompletion = await openAiChatCompletionWithLogging([\n    {\n      role: 'system',\n      content: `you are an expert web browsing AI. you were given the original goal prompt:\"${originalPrompt}\"\nYou are on URL: ${currentURL}\nThis is the plan so far:\n  ${JSON.stringify(originalPlan)}\nwe have just finished the final step and want to progress.\nprovide the next step of the plan to successfully achieve the goal and confirm it has been achieved.\nthe response should be in this JSON schema:\n{\n    \"thought\": \"one sentence rationale\",\n    \"action\": \"CLICKBTN\" | \"WAITLOAD\" ,\n    \"ariaLabel\": \"ariaLabelValue\",\n}\nmake sure to use an EXACT aria label from the list of user provided labels\n`,\n    },\n    {\n      role: 'user',\n      content: `nodes: ${JSON.stringify(textOptions)}`,\n    },\n  ])\n  return extractJsonObject(chatCompletion.choices[0].message.content)\n}\n\n/**\n * Sends a prompt to the Plan Reviser to get a revised plan based on the current step and available text options.\n * @param {string} originalPrompt - The original prompt given to the AI.\n * @param {string} originalPlan - The original plan created by the AI.\n * @param {string} currentStep - The current step being executed.\n * @param {string[]} textOptions - An array of user-provided node aria-labels.\n * @returns {Promise<Object>} - A promise that resolves to the revised plan in JSON format.\n */\nexport const sendPromptToPlanReviser = async (\n  originalPrompt,\n  originalPlan,\n  currentStep,\n  textOptions,\n) => {\n  const chatCompletion = await openAiChatCompletionWithLogging([\n    {\n      role: 'system',\n      content: `you are an expert web browsing AI. you were given the original prompt:\"${originalPrompt}\"\nyou originally came up with the plan:\n  ${originalPlan}\nWe currently just tried to execute step of the plan:\n  ${currentStep}\nbut have encountered an issue finding the specified node with aria-label.\nprovide a revised plan continuing from the current step\nthe response should be in this JSON schema:\n{\n  plan: [ {\n    thought: \"one sentence rationale\",\n    action: \"NAVURL\" | \"CLICKBTN\" | \"INPUT\" | \"SELECT\" | \"WAITLOAD\" ,\n    ariaLabel: \"labelName\",\n    param?: \"url\" | \"inputOption\" | \"inputText\"\n  },...]\n}\nONLY use the following user provided nodes aria-labels:\n`,\n    },\n    {\n      role: 'user',\n      content: `nodes: [${textOptions}]`,\n    },\n  ])\n  console.log(chatCompletion.choices[0].message.content)\n  return extractJsonObject(chatCompletion.choices[0].message.content)\n}\n\n/**\n * Sends a prompt to the OpenAI chat completion API to generate a plan for achieving a user goal on a specific URL.\n * @param {string} prompt - The user goal prompt.\n * @param {string} url - The URL of the current page.\n * @returns {Promise<Object>} - A promise that resolves to the extracted JSON object containing the generated plan.\n */\nexport const sendPromptToPlanner = async (prompt, url) => {\n  const chatCompletion = await openAiChatCompletionWithLogging([\n    {\n      role: 'system',\n      content: `you are an expert web browsing AI.\ngiven a user goal prompt devise a plan to achieve the goal.\nAssume we are already on the URL: ${url} and give the following steps.\nProvide the response with this JSON schema:\n{\n  plan: [ {\n    \"thought\": \"one sentence rationale\",\n    \"action\": \"NAVURL\" | \"CLICKBTN\" | \"INPUT\" | \"SELECT\" | \"WAITLOAD\" ,\n    \"ariaLabel\": \"labelName\",\n    \"param\"?: \"url\" | \"inputOption\" | \"inputText\"\n  },...]\n}\n`,\n    },\n    {\n      role: 'user',\n      content: `Your goal is: ${prompt}`,\n    },\n  ])\n  console.log(chatCompletion.choices[0].message.content)\n  return extractJsonObject(chatCompletion.choices[0].message.content)\n}\n\n/**\n * Checks if any candidate prompts match or closely align with a given goal prompt using OpenAI's chat completion API.\n * @param {string} prompt - The goal prompt to compare against candidate prompts.\n * @param {string[]} candidates - An array of candidate prompts to check for a match.\n * @returns {Promise<{match: string}>} A promise that resolves to an object with a 'match' property containing the matching candidate prompt or an empty string if no match is found.\n */\nexport const checkCandidatePrompts = async (prompt, candidates) => {\n  const chatCompletion = await openAiChatCompletionWithLogging([\n    {\n      role: 'system',\n      content: `Compare the following long goal prompt with the given list of smaller candidate prompts.\nDetermine if any of the smaller prompts match or closely align with the intention or key elements of the long goal prompt.\nIf a match is found, identify and return the matching smaller prompt.\nProvide response in this JSON schema:\n{\n  match: \"candidate prompt\" | \"\"\n}\n`,\n    },\n    {\n      role: 'user',\n      content: `goal prompt: ${prompt}\ncandidate prompts: ${candidates}`,\n    },\n  ])\n  console.log(chatCompletion.choices[0].message.content)\n  return extractJsonObject(chatCompl... (1674 more characters)"
        }
      },
      "id": 2,
      "thread_id": 0,
      "parent_id": 1,
      "elapsed": 0.0018869189952965826
    }
  ],
  "version": "1.12",
  "metadata": {
    "client": {
      "name": "appmap-node",
      "version": "2.23.0",
      "url": "https://github.com/getappmap/appmap-node"
    },
    "language": {
      "name": "javascript",
      "engine": "Node.js",
      "version": "v18.19.0"
    },
    "app": "clickolas-cage",
    "recorder": {
      "type": "requests",
      "name": "requests"
    },
    "name": "GET /src/llm-utils.js (200) â€” 2024-06-12T19:14:31.667Z"
  },
  "classMap": [
    {
      "type": "http",
      "name": "HTTP server requests",
      "children": [
        {
          "type": "route",
          "name": "GET /src/llm-utils.js"
        }
      ]
    }
  ]
}